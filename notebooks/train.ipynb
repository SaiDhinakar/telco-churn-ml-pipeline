{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49e53b5",
   "metadata": {},
   "source": [
    "# Training and Testing the different ML models for telco customer churn perdiction\n",
    "\n",
    "\n",
    "Train and track multiple models with MLflow, such as:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "Compare model performance using metrics such as AUC, F1-score, and Accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb312e1",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285cbea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8518de0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SeniorCitizen",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Partner",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dependents",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tenure",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PhoneService",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "InternetService",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "OnlineSecurity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "OnlineBackup",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DeviceProtection",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TechSupport",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "StreamingTV",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "StreamingMovies",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Contract",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PaperlessBilling",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PaymentMethod",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "MonthlyCharges",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TotalCharges",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Churn",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1676f5f7-5d4d-4024-9a11-910b31ae6347",
       "rows": [
        [
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "29.85",
         "29.85",
         "0"
        ],
        [
         "1",
         "0",
         "0",
         "0",
         "34",
         "1",
         "1",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "56.95",
         "1889.5",
         "0"
        ],
        [
         "2",
         "0",
         "0",
         "0",
         "2",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1",
         "53.85",
         "108.15",
         "1"
        ],
        [
         "3",
         "0",
         "0",
         "0",
         "45",
         "0",
         "1",
         "1",
         "0",
         "1",
         "1",
         "0",
         "0",
         "1",
         "0",
         "2",
         "42.3",
         "1840.75",
         "0"
        ],
        [
         "4",
         "0",
         "0",
         "0",
         "2",
         "1",
         "2",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "70.7",
         "151.65",
         "1"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeniorCitizen  Partner  Dependents  tenure  PhoneService  InternetService  \\\n",
       "0              0        1           0       1             0                1   \n",
       "1              0        0           0      34             1                1   \n",
       "2              0        0           0       2             1                1   \n",
       "3              0        0           0      45             0                1   \n",
       "4              0        0           0       2             1                2   \n",
       "\n",
       "   OnlineSecurity  OnlineBackup  DeviceProtection  TechSupport  StreamingTV  \\\n",
       "0               0             1                 0            0            0   \n",
       "1               1             0                 1            0            0   \n",
       "2               1             1                 0            0            0   \n",
       "3               1             0                 1            1            0   \n",
       "4               0             0                 0            0            0   \n",
       "\n",
       "   StreamingMovies  Contract  PaperlessBilling  PaymentMethod  MonthlyCharges  \\\n",
       "0                0         0                 1              0           29.85   \n",
       "1                0         1                 0              1           56.95   \n",
       "2                0         0                 1              1           53.85   \n",
       "3                0         1                 0              2           42.30   \n",
       "4                0         0                 1              0           70.70   \n",
       "\n",
       "   TotalCharges  Churn  \n",
       "0         29.85      0  \n",
       "1       1889.50      0  \n",
       "2        108.15      1  \n",
       "3       1840.75      0  \n",
       "4        151.65      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load cleanded dataset\n",
    "data = pd.read_csv(os.path.join(\"..\", \"data\", \"processed\", \"cleaned_telco_customer_churn.csv\"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4e3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Churn']\n",
    "features = data.drop(columns=['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f710bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42, stratify=target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aabf67",
   "metadata": {},
   "source": [
    "# Define Models and Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ca9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10420a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'logistic_regression': {\n",
    "        'model': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('logreg', LogisticRegression())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'logreg__solver': ['liblinear', 'lbfgs']\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30]\n",
    "        }\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(),\n",
    "        'params': {\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237edcf5",
   "metadata": {},
   "source": [
    "## Train different models and track it in mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a312e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4a2808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/19 00:27:02 INFO mlflow.tracking.fluent: Experiment with name 'Telco_Churn_Models' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_experiment(\"Telco_Churn_Models\")\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "310b6a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training and logging to MLflow...\n",
      "Training logistic_regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression: {'logreg__C': 0.01, 'logreg__solver': 'liblinear'}\n",
      "logistic_regression - Accuracy: 0.7920511000709723, Precision: 0.6246153846153846, Recall: 0.5427807486631016, F1 Score: 0.580829756795422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/19 00:27:14 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/19 00:27:33 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/10/19 00:27:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run logistic_regression at: http://localhost:5000/#/experiments/878427477543592209/runs/2421c33e761243c1b0bf8c9a676130d0\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/878427477543592209\n",
      "logistic_regression - F1 Score: 0.580829756795422\n",
      "Training random_forest...\n",
      "Best parameters for random_forest: {'max_depth': 10, 'n_estimators': 100}\n",
      "random_forest - Accuracy: 0.794889992902768, Precision: 0.6366559485530546, Recall: 0.5294117647058824, F1 Score: 0.5781021897810219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/19 00:27:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/19 00:28:01 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/10/19 00:28:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run random_forest at: http://localhost:5000/#/experiments/878427477543592209/runs/28d052635d204c6e87a7f40c7c157624\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/878427477543592209\n",
      "random_forest - F1 Score: 0.5781021897810219\n",
      "Training xgboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:05] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:06] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:07] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:08] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:10] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:11] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:28:13] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for xgboost: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}\n",
      "xgboost - Accuracy: 0.7955997161107168, Precision: 0.6433333333333333, Recall: 0.516042780748663, F1 Score: 0.5727002967359051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/19 00:28:13 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "/mnt/sda1/ML-Projects/telco-churn-mlops-pipeline/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1115: UserWarning: [00:28:13] WARNING: /workspace/src/c_api/c_api.cc:1570: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "2025/10/19 00:28:21 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/10/19 00:28:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run xgboost at: http://localhost:5000/#/experiments/878427477543592209/runs/f27a34342f0141f599680b3a97496fdc\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/878427477543592209\n",
      "xgboost - F1 Score: 0.5727002967359051\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615[LightGBM] [Info] Total Bins 615\n",
      "\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 17\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265306 -> initscore=-1.018570\n",
      "[LightGBM] [Info] Start training from score -1.018570\n",
      "[LightGBM] [Info] Number of positive: 1495, number of negative: 4139\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 615\n",
      "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265353 -> initscore=-1.018328\n",
      "[LightGBM] [Info] Start training from score -1.018328\n",
      "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 50, 'num_leaves': 31}\n",
      "LightGBM - Accuracy: 0.7970191625266146, Precision: 0.6392405063291139, Recall: 0.5401069518716578, F1 Score: 0.5855072463768116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/19 00:29:00 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/19 00:29:16 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/10/19 00:29:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run LightGBM at: http://localhost:5000/#/experiments/878427477543592209/runs/2515a1afbfc94130836890cfe476c843\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/878427477543592209\n",
      "LightGBM - F1 Score: 0.5855072463768116\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Telco_Churn_Models\")\n",
    "\n",
    "for model_name, mp in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, scoring='f1', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    best_model = clf.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_param(\"best_params\", clf.best_params_)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        if model_name == 'logistic_regression':\n",
    "            mlflow.sklearn.log_model(best_model, artifact_path=\"model\")\n",
    "        elif model_name == 'random_forest':\n",
    "            mlflow.sklearn.log_model(best_model, artifact_path=\"model\")\n",
    "        elif model_name == 'xgboost':\n",
    "            mlflow.xgboost.log_model(best_model, artifact_path=\"model\")\n",
    "        elif model_name == 'LightGBM':\n",
    "            mlflow.lightgbm.log_model(best_model, artifact_path=\"model\")\n",
    "        \n",
    "    print(f\"{model_name} - F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabfe27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telco-churn-mlops-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
